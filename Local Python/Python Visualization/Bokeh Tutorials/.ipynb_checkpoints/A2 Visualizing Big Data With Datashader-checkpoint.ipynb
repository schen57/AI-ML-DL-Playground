{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing big data with datashader\n",
    "Bokeh gets its power by mirroring data from Python (or R) into the web browser. This approach provides full flexibility and interactivity, but because of the way web browsers are designed and built, there are limitations to how much data can be shown in this way. Most web browsers can handle up to about 100,000 or 200,000 datapoints in a Bokeh plot before they will slow down or have memory issues. What do you do when you have larger datasets than that?\n",
    "\n",
    "The datashader library is designed to complement Bokeh by providing visualizations for very large datasets, focusing on faithfully revealing the overall distribution, not just individual data points. datashader installs separately from bokeh, e.g. using conda install datashader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When not to use datashader:\n",
    "\n",
    "1. Plotting less than 1e5 or 1e6 data points\n",
    "2. When every datapoint matters; standard Bokeh will render all of them\n",
    "3. For full interactivity (hover tools) with every datapoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When to use datashader\n",
    "1. Plotting less than 1e5 or 1e6 data points\n",
    "2. When every datapoint matters; standard Bokeh will render all of them\n",
    "3. For full interactivity (hover tools) with every datapoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does datashader work?\n",
    "1. Tools like Bokeh map Data directly into an HTML/JavaScript Plot\n",
    "2. datashader renders Data into a screen-sized Aggregate array, from which an Image can be constructed then embedded into a Bokeh Plot\n",
    "3. Only the fixed-sized Image needs to be sent to the browser, allowing millions or billions of datapoints to be used\n",
    "4. Every step automatically adjusts to the data, but can be customized\n",
    "\n",
    "### The process roughly follows the following:\n",
    "* Data project into a scene\n",
    "* A scene is aggregated/transformed into aggregates\n",
    "* Aggregations are colormapped as an image\n",
    "* image is embedded into the bokeh plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations supported by datashader\n",
    "Datashader currently supports:\n",
    "\n",
    "Scatterplots/heatmaps\n",
    "Time series\n",
    "Connected points (trajectories)\n",
    "Rasters\n",
    "In each case, the output is easily embedded into Bokeh plots, with interactive resampling on pan and zoom, in notebooks or apps. Legends/hover information can be generated from the aggregate arrays, helping provide interactivity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faithfully visualizing big data\n",
    "Once data is large enough that individual points are not easily discerned, it is crucial that the visualization be constructed in a principled way, faithfully revealing the underlying distribution for your visual system to process. For instance, all of these plots show the same data -- is any of them the real distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>val</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>d5</td>\n",
       "      <td>50</td>\n",
       "      <td>-1.397579</td>\n",
       "      <td>0.610189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>d5</td>\n",
       "      <td>50</td>\n",
       "      <td>-2.649610</td>\n",
       "      <td>3.080821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>d5</td>\n",
       "      <td>50</td>\n",
       "      <td>1.933360</td>\n",
       "      <td>0.243676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>d5</td>\n",
       "      <td>50</td>\n",
       "      <td>4.306374</td>\n",
       "      <td>1.032139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>d5</td>\n",
       "      <td>50</td>\n",
       "      <td>-0.493567</td>\n",
       "      <td>-2.242669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cat  val         x         y\n",
       "49995  d5   50 -1.397579  0.610189\n",
       "49996  d5   50 -2.649610  3.080821\n",
       "49997  d5   50  1.933360  0.243676\n",
       "49998  d5   50  4.306374  1.032139\n",
       "49999  d5   50 -0.493567 -2.242669"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "num=10000\n",
    "\n",
    "dists = {cat: pd.DataFrame(dict(x=np.random.normal(x,s,num),\n",
    "                                y=np.random.normal(y,s,num),\n",
    "                                val=val,cat=cat))\n",
    "         for x,y,s,val,cat in \n",
    "         [(2,2,0.01,10,\"d1\"), (2,-2,0.1,20,\"d2\"), (-2,-2,0.5,30,\"d3\"), (-2,2,1.0,40,\"d4\"), (0,0,3,50,\"d5\")]}\n",
    "\n",
    "df = pd.concat(dists,ignore_index=True)\n",
    "df[\"cat\"]=df[\"cat\"].astype(\"category\")\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have 50000 points, 10000 in each of five categories with associated numerical values. This amount of data will be slow to plot directly with Bokeh or any similar libraries that copy the full data into the web browser. Moreover, plotting data of this size with standard approaches has fatal flaws that make the above plots misrepresent the data:\n",
    "\n",
    "Plot A suffers from overplotting, with the distribution obscured by later-plotted datapoints.\n",
    "Plot B uses smaller dots to avoid overplotting,but suffers from oversaturation, with differences in datapoint density not visible because all densities above a certain value show up as the same pure black color\n",
    "Plot C uses transparency to avoid oversaturation, but then suffers from undersaturation, with the 10,000 datapoints in the largest Gaussian (at 0,0) not visible at all.\n",
    "Bokeh can handle 50,000 points, but if the data were larger then these plots would suffer from undersampling, with the distribution not visible or misleading due to too few data points in sparse or zoomed-in regions.\n",
    "Plots A-B also required time-consuming and error-prone manual tweaking of parameters, which is problematic if the data is large enough that the visualization is the main way for us to understand the data.\n",
    "\n",
    "Using datashader, we can avoid all of these problems by rendering the data to an intermediate array that allows automatic ranging in all dimensions, revealing the true distribution with no parameter tweaking and very little code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datashader as ds\n",
    "import datashader.glyphs\n",
    "import datashader.transfer_functions as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python3.6.3 |Anaconda custom (64-bit)| (default, Nov  8 2017, 18:10:31) \n",
      "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Python\"+ sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
